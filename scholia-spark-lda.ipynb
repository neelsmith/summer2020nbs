{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark LDA\n",
    "\n",
    "An example of topic modelling a corpus of texts using Spark ML's LDA.\n",
    "\n",
    "In the first two code cells, you can define your main decisions about how to topic model your corpus by setting key values, and by downloading and cleaning up your texts.\n",
    "\n",
    "\n",
    "## Settings\n",
    "\n",
    "- `k` is the traditional name for the number of topics to find\n",
    "- `iterations` is the number of cycles the LDA algorithm should run through\n",
    "- `stopWords` is an Array of words to omit from the model\n",
    "- `vocabSize` is the number of terms to consider\n",
    "- `termsToDisplay` is the number of terms to use in describing a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val k = 10\n",
    "val iterations = 20\n",
    "val stopWords = Array(\"de\", \"kai\", \"to\", \"thn\", \"gar\", \"twn\", \"h\", \"tou\", \"ws\", \"o\", \"ths\", \"ton\", \"dia\", \"mh\", \"oti\", \"ou\", \"pros\", \"eis\", \"men\", \"oi\", \"ouk\", \"en\", \"tous\", \"epi\", \"ta\", \"tw|\", \"tois\", \"auton\", \"ei\", \"nun\", \"peri\", \"hn\", \"oun\", \"autw|\", \"autou\", \"alla\", \"tas\", \"all'\", \"esti\", \"estin\", \"te\", \"th|\", \"touto\", \"tauta\", \"apo\", \"ek\", \"meta\", \"ti\", \"ec\", \"anti\", \"oude\")\n",
    "\n",
    "val vocabSize = 10000\n",
    "val minimumTokenLength = 4\n",
    "val termsToDisplay = 15\n",
    "\n",
    "// Cosmetic setting for table display:\n",
    "val maxWidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and clean up text\n",
    "\n",
    "\n",
    "This example uses delimited-text data from the OCRE data set. \n",
    "We extract column 7, then tidy up the data by:\n",
    "\n",
    "- converting all text to lower case\n",
    "- removing all characters *except* alphabetic `a-z` and the space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val personalRepo = coursierapi.MavenRepository.of(\"https://dl.bintray.com/neelsmith/maven\")\n",
    "interp.repositories() ++= Seq(personalRepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`edu.holycross.shot.cite::xcite:4.3.0`\n",
    "import $ivy.`edu.holycross.shot::ohco2:10.20.3`\n",
    "import $ivy.`edu.holycross.shot::greek:5.5.1`\n",
    "import $ivy.`edu.holycross.shot.mid::orthography:2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edu.holycross.shot.cite._\n",
    "import edu.holycross.shot.ohco2._\n",
    "import edu.holycross.shot.greek._\n",
    "import edu.holycross.shot.mid.orthography._\n",
    "\n",
    "\n",
    "val venetusAUrl = \"https://raw.githubusercontent.com/neelsmith/summer2020nbs/master/data/hmt-2020i-noIliad.cex\"\n",
    "val twins9Url = \"https://raw.githubusercontent.com/neelsmith/summer2020nbs/master/data/twins9corpus.cex\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create  source corpora\n",
    "val twins9 = CorpusSource.fromUrl(twins9Url)\n",
    "val venetusA = CorpusSource.fromUrl(venetusAUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// - tokenize, keep only lexical tokens\n",
    "// - make LiteraryGreekStrings from lexical tokens, drop accents and breathings\n",
    "// - recompose into a single stripped-down string for each line\n",
    "def curateNode(cn: CitableNode, siglum: String) : CitableNode = {\n",
    "  if (cn.text.isEmpty){\n",
    "    println(\"EMPTY TEXT: \" + cn.urn)\n",
    "    cn\n",
    "  } else {\n",
    "\n",
    "    val lexTokens = LiteraryGreekString.tokenizeNode(cn).filter(_.tokenCategory == Some(LexicalToken))\n",
    "    val lgs = lexTokens.map(tkn => LiteraryGreekString(tkn.text).toLower.stripBreathingAccent.ascii)\n",
    "    val simpleAscii = lgs.mkString(\" \")\n",
    "    CitableNode(cn.urn.addVersion(s\"${siglum}_simpleascii\"),simpleAscii)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "def asciiCorpus(c: Corpus, siglum: String) : Corpus = {\n",
    "  Corpus(c.nodes.map(n => curateNode(n, siglum)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val upsilonScholia = twins9 ~~ CtsUrn(\"urn:cts:greekLit:tlg5026.e3.hmt:\")\n",
    "val vaScholia = venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026:\") \n",
    "val va9 = Corpus(vaScholia.nodes.filter(n => n.urn.passageComponent.startsWith(\"9\")))\n",
    "/*\n",
    "val vaMain = venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026.msA.hmt:\")\n",
    "val vaAim =  venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026.msAim.hmt:\")\n",
    "val vaAint =  venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026.msAint.hmt:\")\n",
    "val vaAext =  venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026.msAext.hmt:\")\n",
    "val vaAilt =  venetusA ~~  CtsUrn(\"urn:cts:greekLit:tlg5026.msAil.hmt:\")\n",
    "*/\n",
    "val allScholia = upsilonScholia ++ va9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScholia.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val scholiaAscii = asciiCorpus(allScholia, \"msa_and_upsilon_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholiaAscii.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Spark notebook session\n",
    "\n",
    "Import libraries, configure debugging, start up a local Spark notebook session.  These four cells fall in the category of \"stuff you copy and paste in to set up a Jupyter notebook with Spark and don't think about too much.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` // Or use any other 2.x version here\n",
    "import org.apache.spark.sql._\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling with Spark LDA\n",
    "\n",
    "After importing a small mountain of Spark libraries, the following cells go through the basic steps of topic modelling:\n",
    "\n",
    "1. Create a text corpus\n",
    "2. Tokenize\n",
    "3. Filter stop words\n",
    "4. Count word occurrences for each text\n",
    "5. Create the LDA model by \"fitting\" it to our data\n",
    "6. Apply the model to compute the topics and their distribution in each document of our corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.clustering.LDA\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create `DataFrame` with text corpus\n",
    "\n",
    "Getting your clean text into a Spark `DataFrame` is an awkward, two-step process. (This should be simpler in futuer versions of Spark.)\n",
    "\n",
    "The important output is `corpus_df`, a `DataFrame` with one row for every text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create RDD:\n",
    "val scholiaText = scholiaAscii.nodes.map(n => n.text)\n",
    "val txtRdd = spark.sparkContext.parallelize(scholiaText).zipWithIndex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import implicits *after* creation of context.\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val corpus_df = txtRdd.toDF(\"corpus\", \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we can paste it this handy snippet defining a function that will beautify our display of Spark `DataFrame`s in HTML.  (We'll use the `showHTML` function later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// based on a snippet by Ivan Zaitsev\n",
    "// https://github.com/almond-sh/almond/issues/180#issuecomment-364711999\n",
    "implicit class RichDF(val df: DataFrame) {\n",
    "  def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "    import xml.Utility.escape\n",
    "    val data = df.take(limit)\n",
    "    val header = df.schema.fieldNames.toSeq\n",
    "    val rows: Seq[Seq[String]] = data.map { row =>\n",
    "      row.toSeq.map { cell =>\n",
    "        val str = cell match {\n",
    "          case null => \"null\"\n",
    "          case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "          case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "          case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "          case _ => cell.toString\n",
    "        }\n",
    "        if (truncate > 0 && str.length > truncate) {\n",
    "          // do not show ellipses for strings shorter than 4 characters.\n",
    "          if (truncate < 4) str.substring(0, truncate)\n",
    "          else str.substring(0, truncate - 3) + \"...\"\n",
    "        } else {\n",
    "          str\n",
    "        }\n",
    "      }: Seq[String]\n",
    "    }\n",
    "\n",
    "    publish.html(s\"\"\"\n",
    "      <table class=\"table\">\n",
    "        <tr>\n",
    "        ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "        </tr>\n",
    "        ${rows.map { row =>\n",
    "          s\"<tr>${row.map { c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "        }.mkString\n",
    "        }\n",
    "      </table>\"\"\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tokenizer = new RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(minimumTokenLength).setInputCol(\"corpus\").setOutputCol(\"tokens\")\n",
    "val tokenized_df = tokenizer.transform(corpus_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter out stop words\n",
    "\n",
    "Well, think about a serious stop-word list at some point, but here's the technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val remover = new StopWordsRemover().setStopWords(stopWords).setInputCol(\"tokens\").setOutputCol(\"filtered\")\n",
    "val filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute counts of each token for each text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectorizer = new CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").setVocabSize(vocabSize).setMinDF(5).fit(filtered_df)\n",
    "val countVectors = vectorizer.transform(filtered_df).select(\"id\", \"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create (\"fit\") LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lda = new LDA().setK(k).setMaxIter(iterations)\n",
    "val model = lda.fit(countVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compute topics and their distribution in each document\n",
    "\n",
    "Each topic is a set of terms with corresponding weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topics = model.describeTopics(termsToDisplay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.showHTML(truncate=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label topics\n",
    "\n",
    "Digression:  for human readers, we'll replace index numbers for each term with the actual term.\n",
    "\n",
    "1. Create a new DataFrame with ordered lists ot terms by looking up the term for each term index.\n",
    "2. Number the rows of this DataFrame so we can join it with the existing topic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicLabels = topics.select(\"termIndices\").map { case Row(r:  WrappedArray[Integer]) => r.map( i => vectorizer.vocabulary(i) ) }\n",
    "val labelsNumberedLong = topicLabels.rdd.zipWithIndex.toDF(\"terms\", \"topicLong\")\n",
    "val labelsIndexed = labelsNumberedLong.withColumn(\"topic\", $\"topicLong\".cast(IntegerType)).drop(\"topicLong\")\n",
    "\n",
    "val topicsWithTerms = labelsIndexed.join(topics, labelsIndexed.col(\"topic\") === topics.col(\"topic\")).drop(labelsIndexed.col(\"topic\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val weightedLabels = topicsWithTerms.withColumn(\"termsWithWeight\", expr(\"zip_with(terms, termWeights, (t,w) -> concat(t, ' ', w))\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Flat view\n",
    "weightedLabels.select(\"topic\", \"termsWithWeight\").showHTML(truncate=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exploded view\n",
    "val explodedTerms = weightedLabels.select(col(\"*\"),explode(col(\"termsWithWeight\"))).select(\"topic\",\"col\")\n",
    "\n",
    "explodedTerms.showHTML(explodedTerms.count.toInt, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distribution of topics per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val transformed = model.transform(countVectors)\n",
    "transformed.printSchema // show(false)\n",
    "transformed.showHTML(3, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 91% of document 2 (the third document) is assigned to topic 5.  Let's compare the contents of document 2 and the topic definition.\n",
    "\n",
    "We can just index directly into our original Array of texts to see document 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsilonScholiaText(2)\n",
    "upsilonScholiaAscii.nodes(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a condition on the `weightedLabels` data frame to filter it to topic 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topic5 = weightedLabels.filter(weightedLabels(\"topic\") === 5).select(\"termsWithWeight\") //.showHTML(truncate=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break the resulting array out to one element per line with Spark's `explode` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic5.select( explode(col(\"termsWithWeight\"))).showHTML(truncate=maxWidth)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "nteract": {
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
